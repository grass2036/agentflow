"""
OpenRouter API Integration
Provides access to 325+ AI models through OpenRouter's unified API
"""

import json
import logging
import urllib.request
import urllib.parse
import urllib.error
from typing import Dict, List, Any, Optional
from datetime import datetime

from ..core.types import AgentRole

logger = logging.getLogger(__name__)

class OpenRouterIntegration:
    """OpenRouter APIé›†æˆç±»"""
    
    def __init__(self, api_key: str, app_name: str = "AI Agent Orchestrator", 
                 enable_premium_models: bool = False):
        self.api_key = api_key
        self.base_url = "https://openrouter.ai/api/v1"
        self.app_name = app_name
        self.enable_premium_models = enable_premium_models
        self.headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json",
            "HTTP-Referer": "https://ai-agent-orchestrator.com",
            "X-Title": app_name,
        }
        
        # Model preferences for different agent roles
        self.agent_model_preferences = {
            AgentRole.PROJECT_MANAGER: {
                "premium": "gpt-3.5-turbo",
                "free": "x-ai/grok-4-fast:free",
                "fallback": "google/gemma-2-9b-it:free", 
                "system_prompt": """ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œçš„é¡¹ç›®ç»ç†ã€‚ä½ çš„èŒè´£æ˜¯ï¼š
- åˆ†æé¡¹ç›®éœ€æ±‚å’Œå¤æ‚åº¦
- åˆ¶å®šè¯¦ç»†çš„é¡¹ç›®è®¡åˆ’å’Œæ—¶é—´çº¿
- è¯†åˆ«æ½œåœ¨é£é™©å’Œä¾èµ–å…³ç³»
- åˆ†é…èµ„æºå’Œåè°ƒå›¢é˜Ÿ
- ç›‘æ§é¡¹ç›®è¿›åº¦å’Œè´¨é‡

è¯·ç”¨ä¸“ä¸šã€ç»“æ„åŒ–çš„æ–¹å¼å›åº”ï¼ŒåŒ…å«å…·ä½“çš„è¡ŒåŠ¨è®¡åˆ’å’Œé‡Œç¨‹ç¢‘ã€‚"""
            },
            
            AgentRole.ARCHITECT: {
                "premium": "deepseek/deepseek-v3.1-terminus",
                "free": "x-ai/grok-4-fast:free",
                "fallback": "qwen/qwen3-coder-flash", 
                "system_prompt": """ä½ æ˜¯ä¸€ä½èµ„æ·±çš„ç³»ç»Ÿæ¶æ„å¸ˆã€‚ä½ çš„ä¸“é•¿åŒ…æ‹¬ï¼š
- è®¾è®¡å¯æ‰©å±•çš„ç³»ç»Ÿæ¶æ„
- é€‰æ‹©åˆé€‚çš„æŠ€æœ¯æ ˆå’Œè®¾è®¡æ¨¡å¼
- å®šä¹‰APIæ¥å£å’Œæ•°æ®æ¨¡å‹
- è€ƒè™‘æ€§èƒ½ã€å®‰å…¨æ€§å’Œå¯ç»´æŠ¤æ€§
- åˆ¶å®šæŠ€æœ¯è§„èŒƒå’Œæœ€ä½³å®è·µ

è¯·æä¾›è¯¦ç»†çš„æ¶æ„è®¾è®¡ï¼ŒåŒ…å«å›¾è¡¨ã€æŠ€æœ¯é€‰å‹ç†ç”±å’Œå®æ–½å»ºè®®ã€‚"""
            },
            
            AgentRole.BACKEND_DEVELOPER: {
                "premium": "qwen/qwen3-coder-plus",
                "free": "x-ai/grok-4-fast:free",
                "fallback": "deepseek/deepseek-chat-v3.1:free",
                "system_prompt": """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„åç«¯å¼€å‘å·¥ç¨‹å¸ˆã€‚ä½ çš„æŠ€èƒ½åŒ…æ‹¬ï¼š
- ç¼–å†™é«˜è´¨é‡çš„åç«¯ä»£ç 
- è®¾è®¡å’Œå®ç°APIæ¥å£
- æ•°æ®åº“è®¾è®¡å’Œä¼˜åŒ–
- å¤„ç†å¹¶å‘ã€ç¼“å­˜å’Œæ€§èƒ½ä¼˜åŒ–
- å®ç°å®‰å…¨è®¤è¯å’Œæˆæƒ

è¯·æä¾›å®Œæ•´ã€å¯è¿è¡Œçš„ä»£ç å®ç°ï¼ŒåŒ…å«é”™è¯¯å¤„ç†ã€ç±»å‹æ³¨è§£å’Œæ–‡æ¡£ã€‚"""
            },
            
            AgentRole.FRONTEND_DEVELOPER: {
                "premium": "gpt-3.5-turbo",
                "free": "x-ai/grok-4-fast:free",
                "fallback": "google/gemma-2-9b-it:free",
                "system_prompt": """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å‰ç«¯å¼€å‘å·¥ç¨‹å¸ˆã€‚ä½ çš„ä¸“é•¿æ˜¯ï¼š
- æ„å»ºå“åº”å¼ç”¨æˆ·ç•Œé¢
- å®ç°ç°ä»£å‰ç«¯æ¡†æ¶(React, Vue, Angular)
- ç”¨æˆ·ä½“éªŒè®¾è®¡å’Œäº¤äº’ä¼˜åŒ–
- å‰ç«¯æ€§èƒ½ä¼˜åŒ–å’ŒSEO
- ç§»åŠ¨ç«¯é€‚é…å’ŒPWAå¼€å‘

è¯·æä¾›å®Œæ•´çš„å‰ç«¯å®ç°ï¼ŒåŒ…å«ç»„ä»¶åŒ–è®¾è®¡å’Œæœ€ä½³å®è·µã€‚"""
            },
            
            AgentRole.QA_ENGINEER: {
                "premium": "gpt-3.5-turbo",
                "free": "x-ai/grok-4-fast:free",
                "fallback": "nvidia/nemotron-nano-9b-v2:free",
                "system_prompt": """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„QAæµ‹è¯•å·¥ç¨‹å¸ˆã€‚ä½ çš„èŒè´£åŒ…æ‹¬ï¼š
- è®¾è®¡å…¨é¢çš„æµ‹è¯•ç­–ç•¥å’Œæµ‹è¯•ç”¨ä¾‹
- å®æ–½è‡ªåŠ¨åŒ–æµ‹è¯•æ¡†æ¶
- æ‰§è¡ŒåŠŸèƒ½ã€æ€§èƒ½å’Œå®‰å…¨æµ‹è¯•
- ç¼ºé™·è·Ÿè¸ªå’Œè´¨é‡è¯„ä¼°
- æµ‹è¯•æ•°æ®ç®¡ç†å’Œç¯å¢ƒé…ç½®

è¯·æä¾›è¯¦ç»†çš„æµ‹è¯•è®¡åˆ’ã€æµ‹è¯•è„šæœ¬å’Œè´¨é‡ä¿è¯å»ºè®®ã€‚"""
            },
            
            AgentRole.DEVOPS_ENGINEER: {
                "premium": "deepseek/deepseek-v3.1-terminus",
                "free": "x-ai/grok-4-fast:free",
                "fallback": "qwen/qwen3-coder-flash",
                "system_prompt": """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„DevOpså·¥ç¨‹å¸ˆã€‚ä½ çš„ä¸“ä¸šé¢†åŸŸåŒ…æ‹¬ï¼š
- CI/CDæµæ°´çº¿è®¾è®¡å’Œå®æ–½
- å®¹å™¨åŒ–å’Œå¾®æœåŠ¡éƒ¨ç½²
- äº‘å¹³å°æ¶æ„å’ŒåŸºç¡€è®¾æ–½å³ä»£ç 
- ç›‘æ§ã€æ—¥å¿—å’Œå‘Šè­¦ç³»ç»Ÿ
- å®‰å…¨æ‰«æå’Œåˆè§„æ€§æ£€æŸ¥

è¯·æä¾›å®Œæ•´çš„éƒ¨ç½²æ–¹æ¡ˆï¼ŒåŒ…å«é…ç½®æ–‡ä»¶ã€è„šæœ¬å’Œè¿ç»´å»ºè®®ã€‚"""
            },
            
            AgentRole.UI_UX_DESIGNER: {
                "premium": "gpt-3.5-turbo",
                "free": "x-ai/grok-4-fast:free",
                "fallback": "google/gemma-2-9b-it:free",
                "system_prompt": """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„UI/UXè®¾è®¡å¸ˆã€‚ä½ çš„è®¾è®¡ç†å¿µåŒ…æ‹¬ï¼š
- ç”¨æˆ·ä½“éªŒç ”ç©¶å’Œè®¾è®¡æ€ç»´
- ç•Œé¢è®¾è®¡å’Œäº¤äº’åŸå‹
- å¯ç”¨æ€§æµ‹è¯•å’Œç”¨æˆ·åé¦ˆæ”¶é›†
- è®¾è®¡ç³»ç»Ÿå’Œç»„ä»¶åº“æ„å»º
- å“åº”å¼è®¾è®¡å’Œæ— éšœç¢è®¿é—®

è¯·æä¾›è¯¦ç»†çš„è®¾è®¡æ–¹æ¡ˆï¼ŒåŒ…å«ç”¨æˆ·æµç¨‹ã€ç•Œé¢æ¨¡å‹å’Œè®¾è®¡è§„èŒƒã€‚"""
            },
            
            AgentRole.DATA_ENGINEER: {
                "premium": "qwen/qwen3-coder-plus",
                "free": "x-ai/grok-4-fast:free",
                "fallback": "deepseek/deepseek-chat-v3.1:free",
                "system_prompt": """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ•°æ®å·¥ç¨‹å¸ˆã€‚ä½ çš„æŠ€æœ¯æ ˆåŒ…æ‹¬ï¼š
- æ•°æ®ç®¡é“è®¾è®¡å’ŒETLæµç¨‹
- å¤§æ•°æ®å¤„ç†å’Œæ•°æ®ä»“åº“æ„å»º
- æ•°æ®è´¨é‡ç›‘æ§å’Œæ²»ç†
- å®æ—¶æ•°æ®æµå¤„ç†
- æ•°æ®å®‰å…¨å’Œéšç§ä¿æŠ¤

è¯·æä¾›å®Œæ•´çš„æ•°æ®è§£å†³æ–¹æ¡ˆï¼ŒåŒ…å«æ¶æ„è®¾è®¡ã€ä»£ç å®ç°å’Œæ•°æ®æ²»ç†ç­–ç•¥ã€‚"""
            },
            
            AgentRole.SECURITY_ENGINEER: {
                "premium": "gpt-3.5-turbo",
                "free": "x-ai/grok-4-fast:free",
                "fallback": "deepseek/deepseek-v3.1-terminus",
                "system_prompt": """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å®‰å…¨å·¥ç¨‹å¸ˆã€‚ä½ çš„å®‰å…¨ä¸“é•¿åŒ…æ‹¬ï¼š
- å®‰å…¨å¨èƒå»ºæ¨¡å’Œé£é™©è¯„ä¼°
- å®‰å…¨ä»£ç å®¡æŸ¥å’Œæ¼æ´æ‰«æ
- èº«ä»½è®¤è¯å’Œè®¿é—®æ§åˆ¶
- å®‰å…¨ç›‘æ§å’Œäº‹ä»¶å“åº”
- åˆè§„æ€§æ£€æŸ¥å’Œå®‰å…¨åŸ¹è®­

è¯·æä¾›å…¨é¢çš„å®‰å…¨æ–¹æ¡ˆï¼ŒåŒ…å«å¨èƒåˆ†æã€é˜²æŠ¤æªæ–½å’Œåº”æ€¥é¢„æ¡ˆã€‚"""
            }
        }
        
        # Free models for cost optimization (ordered by priority)
        self.free_models = [
            "x-ai/grok-4-fast:free",          # ğŸ”¥ Best free model - 2M context
            "google/gemma-2-9b-it:free",      # âœ… Reliable Google model
            "deepseek/deepseek-chat-v3.1:free", # ğŸš€ Good for coding tasks
            "nvidia/nemotron-nano-9b-v2:free",  # âš¡ Fast and efficient
            "openai/gpt-oss-120b:free"        # ğŸ§  Large parameter model
        ]
        
        # Premium models (disabled by default)
        self.premium_models = [
            "gpt-4",
            "gpt-3.5-turbo", 
            "claude-3-opus",
            "claude-3-sonnet",
            "deepseek/deepseek-v3.1-terminus",
            "qwen/qwen3-coder-plus"
        ]
        
        logger.info(f"OpenRouter integration initialized with {len(self.agent_model_preferences)} agent configurations")
    
    def make_request(self, endpoint: str, method: str = "GET", data: dict = None) -> Dict[str, Any]:
        """å‘èµ·HTTPè¯·æ±‚"""
        url = f"{self.base_url}/{endpoint}"
        
        try:
            req = urllib.request.Request(url, method=method)
            
            for key, value in self.headers.items():
                req.add_header(key, value)
            
            if data and method == "POST":
                json_data = json.dumps(data).encode('utf-8')
                req.data = json_data
            
            with urllib.request.urlopen(req, timeout=60) as response:
                response_data = response.read().decode('utf-8')
                return {
                    "success": True,
                    "status": response.status,
                    "data": json.loads(response_data)
                }
                
        except urllib.error.HTTPError as e:
            error_data = e.read().decode('utf-8')
            logger.error(f"OpenRouter HTTP error {e.code}: {error_data}")
            return {
                "success": False,
                "status": e.code,
                "error": f"HTTP {e.code}: {error_data}"
            }
        except Exception as e:
            logger.error(f"OpenRouter request failed: {str(e)}")
            return {
                "success": False,
                "status": 0,
                "error": f"Request failed: {str(e)}"
            }
    
    def get_available_models(self) -> Dict[str, Any]:
        """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
        logger.info("Fetching available models from OpenRouter")
        result = self.make_request("models")
        
        if result["success"]:
            models = result["data"].get("data", [])
            logger.info(f"Retrieved {len(models)} models from OpenRouter")
            return {
                "success": True,
                "models": models,
                "free_models": [m for m in models if m.get('id', '').endswith(':free')],
                "total_count": len(models)
            }
        else:
            return {
                "success": False,
                "error": result["error"]
            }
    
    def chat_completion(self, messages: List[Dict[str, str]], model: str = "gpt-3.5-turbo", 
                       max_tokens: int = 2000, temperature: float = 0.7, **kwargs) -> Dict[str, Any]:
        """æ‰§è¡ŒèŠå¤©å®Œæˆ"""
        payload = {
            "model": model,
            "messages": messages,
            "max_tokens": max_tokens,
            "temperature": temperature,
            **kwargs
        }
        
        logger.info(f"Making chat completion request with model: {model}")
        result = self.make_request("chat/completions", method="POST", data=payload)
        
        if result["success"]:
            response_data = result["data"]
            usage = response_data.get("usage", {})
            
            logger.info(f"Chat completion successful. Tokens used: {usage.get('total_tokens', 0)}")
            return {
                "success": True,
                "response": response_data,
                "content": response_data.get("choices", [{}])[0].get("message", {}).get("content", ""),
                "usage": usage,
                "model_used": model
            }
        else:
            logger.error(f"Chat completion failed: {result['error']}")
            return {
                "success": False,
                "error": result["error"]
            }
    
    def execute_agent_task(self, agent_role: AgentRole, task_description: str, 
                          context: Dict[str, Any] = None, prefer_free: bool = False) -> Dict[str, Any]:
        """æ‰§è¡Œç‰¹å®šè§’è‰²çš„Agentä»»åŠ¡"""
        logger.info(f"Executing agent task for {agent_role.value}: {task_description[:100]}")
        
        # è·å–è¯¥è§’è‰²çš„æ¨¡å‹é…ç½®
        agent_config = self.agent_model_preferences.get(agent_role)
        if not agent_config:
            logger.warning(f"No configuration found for agent role: {agent_role.value}")
            model = "x-ai/grok-4-fast:free"  # Default to best free model
            system_prompt = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ï¼Œè¯·æ ¹æ®è¦æ±‚å®Œæˆä»»åŠ¡ã€‚"
        else:
            # æ™ºèƒ½æ¨¡å‹é€‰æ‹©é€»è¾‘
            if prefer_free or not self.enable_premium_models:
                # ä¼˜å…ˆä½¿ç”¨å…è´¹æ¨¡å‹
                model = agent_config["free"]
                logger.info(f"Using FREE model for {agent_role.value}: {model}")
            else:
                # ä½¿ç”¨ä»˜è´¹æ¨¡å‹
                model = agent_config["premium"] 
                logger.info(f"Using PREMIUM model for {agent_role.value}: {model}")
            
            system_prompt = agent_config["system_prompt"]
        
        # æ„å»ºæ¶ˆæ¯
        messages = [
            {"role": "system", "content": system_prompt}
        ]
        
        # æ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯
        if context:
            context_str = f"é¡¹ç›®ä¸Šä¸‹æ–‡ï¼š\n{json.dumps(context, ensure_ascii=False, indent=2)}\n\n"
            task_description = context_str + task_description
        
        messages.append({"role": "user", "content": task_description})
        
        # æ‰§è¡Œè¯·æ±‚
        start_time = datetime.now()
        result = self.chat_completion(
            messages=messages,
            model=model,
            max_tokens=3000,
            temperature=0.7
        )
        execution_time = (datetime.now() - start_time).total_seconds()
        
        if result["success"]:
            logger.info(f"Agent task completed successfully in {execution_time:.2f}s")
            return {
                "status": "completed",
                "output": result["content"],
                "model_used": result["model_used"],
                "agent_role": agent_role.value,
                "execution_info": {
                    "api_provider": "openrouter",
                    "model": result["model_used"],
                    "execution_time": execution_time,
                    "tokens_used": result["usage"],
                    "timestamp": datetime.now().isoformat()
                }
            }
        else:
            logger.error(f"Agent task failed: {result['error']}")
            
            # æ™ºèƒ½å¤‡ç”¨æ¨¡å‹é‡è¯•é€»è¾‘
            fallback_model = None
            if agent_config:
                if model == agent_config.get("premium") and agent_config.get("free"):
                    # ä»˜è´¹æ¨¡å‹å¤±è´¥ï¼Œå°è¯•å…è´¹æ¨¡å‹
                    fallback_model = agent_config["free"]
                elif model == agent_config.get("free") and agent_config.get("fallback"):
                    # ä¸»è¦å…è´¹æ¨¡å‹å¤±è´¥ï¼Œå°è¯•å¤‡ç”¨å…è´¹æ¨¡å‹
                    fallback_model = agent_config["fallback"]
                
                if fallback_model and fallback_model != model:
                    logger.info(f"Retrying with fallback model: {fallback_model}")
                    retry_result = self.chat_completion(
                        messages=messages,
                        model=fallback_model,
                        max_tokens=3000,
                        temperature=0.7
                    )
                
                if retry_result["success"]:
                    return {
                        "status": "completed",
                        "output": retry_result["content"],
                        "model_used": retry_result["model_used"],
                        "agent_role": agent_role.value,
                        "execution_info": {
                            "api_provider": "openrouter",
                            "model": retry_result["model_used"],
                            "execution_time": execution_time,
                            "tokens_used": retry_result["usage"],
                            "timestamp": datetime.now().isoformat(),
                            "fallback_used": True
                        }
                    }
            
            return {
                "status": "failed",
                "error": result["error"],
                "agent_role": agent_role.value,
                "execution_info": {
                    "api_provider": "openrouter",
                    "model": model,
                    "execution_time": execution_time,
                    "timestamp": datetime.now().isoformat()
                }
            }
    
    def get_model_pricing(self, model_id: str) -> Dict[str, Any]:
        """è·å–æ¨¡å‹å®šä»·ä¿¡æ¯"""
        models_result = self.get_available_models()
        if not models_result["success"]:
            return {"success": False, "error": "Failed to fetch models"}
        
        for model in models_result["models"]:
            if model.get("id") == model_id:
                pricing = model.get("pricing", {})
                return {
                    "success": True,
                    "model_id": model_id,
                    "prompt_price": pricing.get("prompt", "N/A"),
                    "completion_price": pricing.get("completion", "N/A"),
                    "context_length": model.get("context_length", "N/A"),
                    "is_free": model_id.endswith(":free")
                }
        
        return {"success": False, "error": f"Model {model_id} not found"}
    
    def estimate_cost(self, prompt_tokens: int, completion_tokens: int, model_id: str) -> Dict[str, Any]:
        """ä¼°ç®—APIè°ƒç”¨æˆæœ¬"""
        pricing = self.get_model_pricing(model_id)
        if not pricing["success"]:
            return pricing
        
        if pricing["is_free"]:
            return {
                "success": True,
                "total_cost": 0.0,
                "prompt_cost": 0.0,
                "completion_cost": 0.0,
                "currency": "USD",
                "is_free": True
            }
        
        try:
            prompt_price = float(pricing["prompt_price"])
            completion_price = float(pricing["completion_price"])
            
            prompt_cost = (prompt_tokens / 1_000_000) * prompt_price
            completion_cost = (completion_tokens / 1_000_000) * completion_price
            total_cost = prompt_cost + completion_cost
            
            return {
                "success": True,
                "total_cost": round(total_cost, 6),
                "prompt_cost": round(prompt_cost, 6),
                "completion_cost": round(completion_cost, 6),
                "currency": "USD",
                "is_free": False
            }
        except (ValueError, TypeError):
            return {"success": False, "error": "Invalid pricing data"}
    
    def get_recommended_models(self, agent_role: AgentRole = None, prefer_free: bool = False) -> List[str]:
        """è·å–æ¨èæ¨¡å‹åˆ—è¡¨"""
        if agent_role and agent_role in self.agent_model_preferences:
            config = self.agent_model_preferences[agent_role]
            if prefer_free:
                return [config["fallback"], config["primary"]]
            else:
                return [config["primary"], config["fallback"]]
        
        if prefer_free:
            return self.free_models[:5]
        else:
            return ["gpt-3.5-turbo", "deepseek/deepseek-v3.1-terminus", "qwen/qwen3-coder-plus"]
    
    def test_connection(self) -> Dict[str, Any]:
        """æµ‹è¯•APIè¿æ¥"""
        logger.info("Testing OpenRouter API connection")
        
        test_result = self.chat_completion(
            messages=[{"role": "user", "content": "Hello! Please respond with 'Connection successful' if you receive this message."}],
            model="google/gemma-2-9b-it:free",
            max_tokens=50
        )
        
        if test_result["success"]:
            logger.info("OpenRouter API connection test successful")
            return {
                "success": True,
                "message": "OpenRouter API connection successful",
                "response": test_result["content"],
                "model_tested": test_result["model_used"],
                "tokens_used": test_result["usage"]
            }
        else:
            logger.error(f"OpenRouter API connection test failed: {test_result['error']}")
            return {
                "success": False,
                "message": "OpenRouter API connection failed",
                "error": test_result["error"]
            }
    
    def enable_premium_models(self, enable: bool = True) -> None:
        """å¯ç”¨æˆ–ç¦ç”¨ä»˜è´¹æ¨¡å‹"""
        self.enable_premium_models = enable
        status = "enabled" if enable else "disabled"
        logger.info(f"Premium models {status}")
    
    def is_premium_enabled(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å¯ç”¨äº†ä»˜è´¹æ¨¡å‹"""
        return self.enable_premium_models
    
    def get_model_configuration(self) -> Dict[str, Any]:
        """è·å–å½“å‰æ¨¡å‹é…ç½®ä¿¡æ¯"""
        return {
            "premium_models_enabled": self.enable_premium_models,
            "free_models_count": len(self.free_models),
            "premium_models_count": len(self.premium_models),
            "agents_configured": len(self.agent_model_preferences),
            "primary_free_model": self.free_models[0] if self.free_models else None,
            "model_selection_strategy": "premium" if self.enable_premium_models else "free",
            "available_free_models": self.free_models,
            "available_premium_models": self.premium_models if self.enable_premium_models else []
        }
    
    def get_agent_model_info(self, agent_role: AgentRole) -> Dict[str, Any]:
        """è·å–ç‰¹å®šAgentçš„æ¨¡å‹ä¿¡æ¯"""
        agent_config = self.agent_model_preferences.get(agent_role)
        if not agent_config:
            return {"error": f"No configuration found for {agent_role.value}"}
        
        current_model = agent_config["premium"] if self.enable_premium_models else agent_config["free"]
        
        return {
            "agent_role": agent_role.value,
            "current_model": current_model,
            "premium_model": agent_config["premium"],
            "free_model": agent_config["free"],
            "fallback_model": agent_config["fallback"],
            "is_using_premium": self.enable_premium_models,
            "model_costs": {
                "current": "varies" if self.enable_premium_models else "$0.00",
                "premium": "varies",
                "free": "$0.00"
            }
        }
    
    def switch_model_mode(self, mode: str) -> Dict[str, Any]:
        """åˆ‡æ¢æ¨¡å‹æ¨¡å¼"""
        if mode.lower() in ["free", "å…è´¹"]:
            self.enable_premium_models = False
            return {
                "success": True,
                "mode": "free",
                "message": "Switched to FREE models mode - all agents will use free models",
                "cost_impact": "Zero cost operation"
            }
        elif mode.lower() in ["premium", "ä»˜è´¹", "paid"]:
            self.enable_premium_models = True
            return {
                "success": True,
                "mode": "premium", 
                "message": "Switched to PREMIUM models mode - agents will use paid models for better quality",
                "cost_impact": "Charges will apply based on usage"
            }
        else:
            return {
                "success": False,
                "error": f"Invalid mode: {mode}. Use 'free' or 'premium'",
                "current_mode": "premium" if self.enable_premium_models else "free"
            }

# Factory function for easy instantiation
def create_openrouter_integration(api_key: str, enable_premium: bool = False) -> OpenRouterIntegration:
    """åˆ›å»ºOpenRouteré›†æˆå®ä¾‹"""
    return OpenRouterIntegration(api_key, enable_premium_models=enable_premium)
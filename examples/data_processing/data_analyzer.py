#!/usr/bin/env python3
"""
æ•°æ®å¤„ç†ç¤ºä¾‹ï¼šæ•°æ®åˆ†æå™¨
=======================

è¿™ä¸ªç¤ºä¾‹å±•ç¤ºå¦‚ä½•ä½¿ç”¨AgentFlowè¿›è¡Œæ•°æ®åˆ†æå’Œå¤„ç†ã€‚

è¿è¡Œæ–¹å¼ï¼š
python3 examples/data_processing/data_analyzer.py
"""

import asyncio
import json
import csv
import sys
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional
import random
import statistics

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from agentflow.plugins.base import BasePlugin, PluginMetadata, PluginContext


class DataAnalyzerPlugin(BasePlugin):
    """æ•°æ®åˆ†ææ’ä»¶"""
    
    @property
    def metadata(self) -> PluginMetadata:
        return PluginMetadata(
            name="data_analyzer",
            version="1.5.0",
            description="å¼ºå¤§çš„æ•°æ®åˆ†æå’Œç»Ÿè®¡å¤„ç†æ’ä»¶",
            author="AgentFlow Examples",
            tags=["data", "analysis", "statistics", "processing"],
            supports_async=True
        )
    
    def __init__(self, config=None):
        super().__init__(config)
        self.analysis_history = []
        self.processed_datasets = 0
        
    async def initialize(self) -> None:
        print("ğŸ“Š æ•°æ®åˆ†æå™¨åˆå§‹åŒ–ä¸­...")
        print("   æ”¯æŒçš„åˆ†æç±»å‹ï¼šç»Ÿè®¡åˆ†æã€è¶‹åŠ¿åˆ†æã€åˆ†ç»„åˆ†æã€å¼‚å¸¸æ£€æµ‹")
        print("âœ… æ•°æ®åˆ†æå™¨å‡†å¤‡å°±ç»ªï¼")
    
    async def execute_task(self, context: PluginContext) -> Dict[str, Any]:
        """æ‰§è¡Œæ•°æ®åˆ†æä»»åŠ¡"""
        analysis_type = context.data.get("analysis_type", "basic_stats")
        data = context.data.get("data", [])
        
        if not data:
            return {"error": "æ²¡æœ‰æä¾›æ•°æ®", "success": False}
        
        start_time = datetime.now()
        
        try:
            if analysis_type == "basic_stats":
                result = await self._basic_statistics(data)
            elif analysis_type == "trend_analysis":
                result = await self._trend_analysis(data)
            elif analysis_type == "group_analysis":
                result = await self._group_analysis(data, context.data.get("group_by"))
            elif analysis_type == "anomaly_detection":
                result = await self._anomaly_detection(data)
            elif analysis_type == "correlation":
                result = await self._correlation_analysis(data)
            elif analysis_type == "time_series":
                result = await self._time_series_analysis(data)
            else:
                return {"error": f"ä¸æ”¯æŒçš„åˆ†æç±»å‹ï¼š{analysis_type}", "success": False}
            
            # è®°å½•åˆ†æå†å²
            duration = (datetime.now() - start_time).total_seconds()
            self.analysis_history.append({
                "type": analysis_type,
                "timestamp": start_time.isoformat(),
                "duration": duration,
                "data_size": len(data),
                "success": True
            })
            
            self.processed_datasets += 1
            
            result.update({
                "success": True,
                "analysis_type": analysis_type,
                "data_size": len(data),
                "processing_time": f"{duration:.3f}s",
                "processed_datasets": self.processed_datasets
            })
            
            return result
            
        except Exception as e:
            error_result = {
                "error": str(e),
                "success": False,
                "analysis_type": analysis_type
            }
            
            self.analysis_history.append({
                "type": analysis_type,
                "timestamp": start_time.isoformat(),
                "error": str(e),
                "success": False
            })
            
            return error_result
    
    async def _basic_statistics(self, data: List) -> Dict[str, Any]:
        """åŸºç¡€ç»Ÿè®¡åˆ†æ"""
        # æå–æ•°å€¼æ•°æ®
        numeric_data = []
        for item in data:
            if isinstance(item, (int, float)):
                numeric_data.append(item)
            elif isinstance(item, dict):
                for value in item.values():
                    if isinstance(value, (int, float)):
                        numeric_data.append(value)
        
        if not numeric_data:
            return {"error": "æ²¡æœ‰æ‰¾åˆ°æ•°å€¼æ•°æ®"}
        
        stats = {
            "count": len(numeric_data),
            "mean": statistics.mean(numeric_data),
            "median": statistics.median(numeric_data),
            "min": min(numeric_data),
            "max": max(numeric_data),
            "range": max(numeric_data) - min(numeric_data),
            "sum": sum(numeric_data)
        }
        
        # è®¡ç®—æ ‡å‡†å·®ï¼ˆå¦‚æœæ•°æ®é‡è¶³å¤Ÿï¼‰
        if len(numeric_data) > 1:
            stats["std_dev"] = statistics.stdev(numeric_data)
            stats["variance"] = statistics.variance(numeric_data)
        
        # åˆ†ä½æ•°
        stats["quartiles"] = {
            "q1": statistics.quantiles(numeric_data, n=4)[0] if len(numeric_data) >= 4 else None,
            "q2": statistics.median(numeric_data),
            "q3": statistics.quantiles(numeric_data, n=4)[2] if len(numeric_data) >= 4 else None
        }
        
        return {"statistics": stats}
    
    async def _trend_analysis(self, data: List) -> Dict[str, Any]:
        """è¶‹åŠ¿åˆ†æ"""
        if len(data) < 3:
            return {"error": "è¶‹åŠ¿åˆ†æéœ€è¦è‡³å°‘3ä¸ªæ•°æ®ç‚¹"}
        
        # æå–æ•°å€¼åºåˆ—
        values = []
        timestamps = []
        
        for i, item in enumerate(data):
            if isinstance(item, (int, float)):
                values.append(item)
                timestamps.append(i)
            elif isinstance(item, dict) and "value" in item:
                values.append(item["value"])
                timestamps.append(item.get("timestamp", i))
            elif isinstance(item, dict) and "y" in item:
                values.append(item["y"])
                timestamps.append(item.get("x", i))
        
        if len(values) < 3:
            return {"error": "æ²¡æœ‰è¶³å¤Ÿçš„æ•°å€¼æ•°æ®è¿›è¡Œè¶‹åŠ¿åˆ†æ"}
        
        # è®¡ç®—è¶‹åŠ¿
        n = len(values)
        sum_x = sum(range(n))
        sum_y = sum(values)
        sum_xy = sum(i * values[i] for i in range(n))
        sum_x2 = sum(i * i for i in range(n))
        
        # çº¿æ€§å›å½’æ–œç‡
        slope = (n * sum_xy - sum_x * sum_y) / (n * sum_x2 - sum_x * sum_x)
        intercept = (sum_y - slope * sum_x) / n
        
        # è¶‹åŠ¿æ–¹å‘
        if slope > 0.01:
            trend = "ä¸Šå‡"
        elif slope < -0.01:
            trend = "ä¸‹é™"
        else:
            trend = "å¹³ç¨³"
        
        # è®¡ç®—å˜åŒ–ç‡
        changes = [values[i+1] - values[i] for i in range(len(values)-1)]
        avg_change = sum(changes) / len(changes) if changes else 0
        
        return {
            "trend": {
                "direction": trend,
                "slope": slope,
                "intercept": intercept,
                "average_change": avg_change,
                "total_change": values[-1] - values[0],
                "change_percentage": ((values[-1] - values[0]) / values[0] * 100) if values[0] != 0 else 0
            }
        }
    
    async def _group_analysis(self, data: List, group_by: str = None) -> Dict[str, Any]:
        """åˆ†ç»„åˆ†æ"""
        if not group_by:
            return {"error": "éœ€è¦æŒ‡å®šåˆ†ç»„å­—æ®µ"}
        
        if not all(isinstance(item, dict) for item in data):
            return {"error": "åˆ†ç»„åˆ†æéœ€è¦å­—å…¸ç±»å‹çš„æ•°æ®"}
        
        groups = {}
        
        # æŒ‰æŒ‡å®šå­—æ®µåˆ†ç»„
        for item in data:
            if group_by not in item:
                continue
            
            group_key = item[group_by]
            if group_key not in groups:
                groups[group_key] = []
            groups[group_key].append(item)
        
        if not groups:
            return {"error": f"æ²¡æœ‰æ‰¾åˆ°åˆ†ç»„å­—æ®µï¼š{group_by}"}
        
        # åˆ†ææ¯ä¸ªç»„
        group_analysis = {}
        for group_key, group_items in groups.items():
            group_stats = {
                "count": len(group_items),
                "items": group_items[:5]  # åªæ˜¾ç¤ºå‰5é¡¹ä½œä¸ºç¤ºä¾‹
            }
            
            # å¦‚æœç»„å†…æœ‰æ•°å€¼å­—æ®µï¼Œè®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            numeric_fields = {}
            for item in group_items:
                for field, value in item.items():
                    if field != group_by and isinstance(value, (int, float)):
                        if field not in numeric_fields:
                            numeric_fields[field] = []
                        numeric_fields[field].append(value)
            
            # ä¸ºæ¯ä¸ªæ•°å€¼å­—æ®µè®¡ç®—ç»Ÿè®¡
            for field, values in numeric_fields.items():
                if values:
                    group_stats[f"{field}_stats"] = {
                        "count": len(values),
                        "mean": statistics.mean(values),
                        "sum": sum(values),
                        "min": min(values),
                        "max": max(values)
                    }
            
            group_analysis[str(group_key)] = group_stats
        
        return {
            "group_analysis": group_analysis,
            "total_groups": len(groups),
            "group_by": group_by
        }
    
    async def _anomaly_detection(self, data: List) -> Dict[str, Any]:
        """å¼‚å¸¸æ£€æµ‹"""
        # æå–æ•°å€¼æ•°æ®
        numeric_data = []
        indices = []
        
        for i, item in enumerate(data):
            if isinstance(item, (int, float)):
                numeric_data.append(item)
                indices.append(i)
            elif isinstance(item, dict) and "value" in item:
                numeric_data.append(item["value"])
                indices.append(i)
        
        if len(numeric_data) < 5:
            return {"error": "å¼‚å¸¸æ£€æµ‹éœ€è¦è‡³å°‘5ä¸ªæ•°æ®ç‚¹"}
        
        # ä½¿ç”¨IQRæ–¹æ³•æ£€æµ‹å¼‚å¸¸
        mean_val = statistics.mean(numeric_data)
        std_val = statistics.stdev(numeric_data)
        
        # Z-scoreæ–¹æ³•ï¼ˆé˜ˆå€¼ä¸º2.5ï¼‰
        z_score_outliers = []
        for i, value in enumerate(numeric_data):
            z_score = abs(value - mean_val) / std_val if std_val > 0 else 0
            if z_score > 2.5:
                z_score_outliers.append({
                    "index": indices[i],
                    "value": value,
                    "z_score": z_score
                })
        
        # IQRæ–¹æ³•
        if len(numeric_data) >= 4:
            q1 = statistics.quantiles(numeric_data, n=4)[0]
            q3 = statistics.quantiles(numeric_data, n=4)[2]
            iqr = q3 - q1
            lower_bound = q1 - 1.5 * iqr
            upper_bound = q3 + 1.5 * iqr
            
            iqr_outliers = []
            for i, value in enumerate(numeric_data):
                if value < lower_bound or value > upper_bound:
                    iqr_outliers.append({
                        "index": indices[i],
                        "value": value,
                        "reason": "below_lower_bound" if value < lower_bound else "above_upper_bound"
                    })
        else:
            iqr_outliers = []
        
        return {
            "anomaly_detection": {
                "z_score_outliers": z_score_outliers,
                "iqr_outliers": iqr_outliers,
                "total_anomalies": len(set(o["index"] for o in z_score_outliers + iqr_outliers)),
                "data_points": len(numeric_data),
                "anomaly_rate": len(set(o["index"] for o in z_score_outliers + iqr_outliers)) / len(numeric_data) * 100
            }
        }
    
    async def _correlation_analysis(self, data: List) -> Dict[str, Any]:
        """ç›¸å…³æ€§åˆ†æ"""
        if not all(isinstance(item, dict) for item in data):
            return {"error": "ç›¸å…³æ€§åˆ†æéœ€è¦å­—å…¸ç±»å‹çš„æ•°æ®"}
        
        # æ‰¾å‡ºæ‰€æœ‰æ•°å€¼å­—æ®µ
        numeric_fields = {}
        for item in data:
            for field, value in item.items():
                if isinstance(value, (int, float)):
                    if field not in numeric_fields:
                        numeric_fields[field] = []
                    numeric_fields[field].append(value)
        
        if len(numeric_fields) < 2:
            return {"error": "ç›¸å…³æ€§åˆ†æéœ€è¦è‡³å°‘2ä¸ªæ•°å€¼å­—æ®µ"}
        
        # è®¡ç®—ç›¸å…³ç³»æ•°
        correlations = {}
        field_names = list(numeric_fields.keys())
        
        for i in range(len(field_names)):
            for j in range(i + 1, len(field_names)):
                field1, field2 = field_names[i], field_names[j]
                values1, values2 = numeric_fields[field1], numeric_fields[field2]
                
                # ç¡®ä¿ä¸¤ä¸ªå­—æ®µæœ‰ç›¸åŒæ•°é‡çš„æ•°æ®ç‚¹
                min_len = min(len(values1), len(values2))
                if min_len > 1:
                    try:
                        correlation = statistics.correlation(values1[:min_len], values2[:min_len])
                        correlations[f"{field1}_vs_{field2}"] = {
                            "correlation": correlation,
                            "strength": self._interpret_correlation(correlation),
                            "data_points": min_len
                        }
                    except statistics.StatisticsError:
                        correlations[f"{field1}_vs_{field2}"] = {
                            "correlation": 0,
                            "strength": "æ— æ³•è®¡ç®—",
                            "data_points": min_len
                        }
        
        return {"correlation_analysis": correlations}
    
    def _interpret_correlation(self, correlation: float) -> str:
        """è§£é‡Šç›¸å…³ç³»æ•°å¼ºåº¦"""
        abs_corr = abs(correlation)
        if abs_corr >= 0.8:
            return "å¼ºç›¸å…³"
        elif abs_corr >= 0.6:
            return "ä¸­ç­‰ç›¸å…³"
        elif abs_corr >= 0.3:
            return "å¼±ç›¸å…³"
        else:
            return "å¾ˆå¼±æˆ–æ— ç›¸å…³"
    
    async def _time_series_analysis(self, data: List) -> Dict[str, Any]:
        """æ—¶é—´åºåˆ—åˆ†æ"""
        # æœŸæœ›æ•°æ®æ ¼å¼ï¼š[{"timestamp": "...", "value": ...}, ...]
        time_series = []
        
        for item in data:
            if isinstance(item, dict) and "timestamp" in item and "value" in item:
                try:
                    timestamp = datetime.fromisoformat(item["timestamp"].replace('Z', '+00:00'))
                    time_series.append({"timestamp": timestamp, "value": item["value"]})
                except:
                    pass
            elif isinstance(item, dict) and "date" in item and "value" in item:
                try:
                    timestamp = datetime.fromisoformat(str(item["date"]))
                    time_series.append({"timestamp": timestamp, "value": item["value"]})
                except:
                    pass
        
        if len(time_series) < 3:
            return {"error": "æ—¶é—´åºåˆ—åˆ†æéœ€è¦è‡³å°‘3ä¸ªå¸¦æ—¶é—´æˆ³çš„æ•°æ®ç‚¹"}
        
        # æŒ‰æ—¶é—´æ’åº
        time_series.sort(key=lambda x: x["timestamp"])
        
        # è®¡ç®—æ—¶é—´é—´éš”
        intervals = []
        for i in range(1, len(time_series)):
            interval = (time_series[i]["timestamp"] - time_series[i-1]["timestamp"]).total_seconds()
            intervals.append(interval)
        
        # å€¼çš„å˜åŒ–
        values = [item["value"] for item in time_series]
        value_changes = [values[i+1] - values[i] for i in range(len(values)-1)]
        
        # å­£èŠ‚æ€§æ£€æµ‹ï¼ˆç®€å•æ–¹æ³•ï¼‰
        seasonality_detected = False
        if len(values) >= 12:  # éœ€è¦è¶³å¤Ÿçš„æ•°æ®ç‚¹
            # æ£€æŸ¥æ˜¯å¦æœ‰å‘¨æœŸæ€§æ¨¡å¼
            seasonality_detected = self._detect_seasonality(values)
        
        return {
            "time_series_analysis": {
                "data_points": len(time_series),
                "time_span": str(time_series[-1]["timestamp"] - time_series[0]["timestamp"]),
                "average_interval": f"{statistics.mean(intervals):.1f} ç§’" if intervals else "N/A",
                "value_trend": "ä¸Šå‡" if sum(value_changes) > 0 else "ä¸‹é™" if sum(value_changes) < 0 else "å¹³ç¨³",
                "volatility": statistics.stdev(value_changes) if len(value_changes) > 1 else 0,
                "seasonality_detected": seasonality_detected,
                "peak_value": max(values),
                "trough_value": min(values),
                "peak_timestamp": time_series[values.index(max(values))]["timestamp"].isoformat(),
                "trough_timestamp": time_series[values.index(min(values))]["timestamp"].isoformat()
            }
        }
    
    def _detect_seasonality(self, values: List[float]) -> bool:
        """ç®€å•çš„å­£èŠ‚æ€§æ£€æµ‹"""
        # è¿™é‡Œä½¿ç”¨ç®€åŒ–çš„æ–¹æ³•ï¼Œå®é™…åº”ç”¨ä¸­å¯èƒ½éœ€è¦æ›´å¤æ‚çš„ç®—æ³•
        if len(values) < 12:
            return False
        
        # æ£€æŸ¥æ¯å­£åº¦çš„å¹³å‡å€¼æ˜¯å¦æœ‰æ˜¾è‘—å·®å¼‚
        quarter_size = len(values) // 4
        quarters = [
            values[i*quarter_size:(i+1)*quarter_size] 
            for i in range(4)
        ]
        
        quarter_means = [statistics.mean(q) for q in quarters if q]
        
        if len(quarter_means) >= 2:
            # å¦‚æœå­£åº¦é—´å¹³å‡å€¼çš„æ ‡å‡†å·®ç›¸å¯¹è¾ƒå¤§ï¼Œå¯èƒ½å­˜åœ¨å­£èŠ‚æ€§
            mean_of_means = statistics.mean(quarter_means)
            if mean_of_means != 0:
                coefficient_of_variation = statistics.stdev(quarter_means) / mean_of_means
                return coefficient_of_variation > 0.1  # é˜ˆå€¼å¯è°ƒæ•´
        
        return False
    
    def get_analysis_summary(self) -> Dict[str, Any]:
        """è·å–åˆ†ææ€»ç»“"""
        successful_analyses = [h for h in self.analysis_history if h["success"]]
        failed_analyses = [h for h in self.analysis_history if not h["success"]]
        
        return {
            "processed_datasets": self.processed_datasets,
            "total_analyses": len(self.analysis_history),
            "successful_analyses": len(successful_analyses),
            "failed_analyses": len(failed_analyses),
            "success_rate": (len(successful_analyses) / len(self.analysis_history) * 100) 
                          if self.analysis_history else 0,
            "average_processing_time": statistics.mean([h["duration"] for h in successful_analyses]) 
                                     if successful_analyses else 0,
            "analysis_types_used": list(set(h["type"] for h in self.analysis_history))
        }
    
    async def cleanup(self) -> None:
        """æ¸…ç†åˆ†æå™¨"""
        summary = self.get_analysis_summary()
        print(f"\nğŸ“Š æ•°æ®åˆ†æå™¨ä¼šè¯ç»“æŸ")
        print(f"   å¤„ç†æ•°æ®é›†ï¼š{summary['processed_datasets']} ä¸ª")
        print(f"   åˆ†ææˆåŠŸç‡ï¼š{summary['success_rate']:.1f}%")
        print(f"   å¹³å‡å¤„ç†æ—¶é—´ï¼š{summary['average_processing_time']:.3f}ç§’")


async def run_analysis_demo():
    """è¿è¡Œæ•°æ®åˆ†ææ¼”ç¤º"""
    print("ğŸŒŠ AgentFlow æ•°æ®åˆ†æå™¨æ¼”ç¤º")
    print("=" * 60)
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = DataAnalyzerPlugin()
    await analyzer.initialize()
    
    # ç”Ÿæˆç¤ºä¾‹æ•°æ®
    def generate_sample_data():
        """ç”Ÿæˆå¤šç§ç±»å‹çš„ç¤ºä¾‹æ•°æ®"""
        
        # 1. é”€å”®æ•°æ®
        sales_data = []
        base_date = datetime(2024, 1, 1)
        for i in range(30):
            sales_data.append({
                "date": (base_date + timedelta(days=i)).isoformat(),
                "sales": random.randint(1000, 5000) + random.randint(-200, 200),
                "region": random.choice(["åŒ—äº¬", "ä¸Šæµ·", "å¹¿å·", "æ·±åœ³"]),
                "product": random.choice(["äº§å“A", "äº§å“B", "äº§å“C"])
            })
        
        # 2. æ—¶é—´åºåˆ—æ•°æ®
        time_series_data = []
        for i in range(24):
            time_series_data.append({
                "timestamp": (datetime.now() - timedelta(hours=24-i)).isoformat(),
                "value": 100 + 10 * math.sin(i * 0.5) + random.random() * 5
            })
        
        # 3. ç”¨æˆ·æ•°æ®
        user_data = []
        for i in range(100):
            user_data.append({
                "age": random.randint(18, 65),
                "income": random.randint(30000, 200000),
                "city": random.choice(["åŒ—äº¬", "ä¸Šæµ·", "å¹¿å·", "æ·±åœ³", "æ­å·"]),
                "satisfaction": random.randint(1, 10)
            })
        
        return sales_data, time_series_data, user_data
    
    # æ¨¡æ‹Ÿæ•°å­¦å‡½æ•°
    import math
    sales_data, time_series_data, user_data = generate_sample_data()
    
    # åˆ†æä»»åŠ¡åˆ—è¡¨
    analysis_tasks = [
        {
            "name": "åŸºç¡€ç»Ÿè®¡åˆ†æ - é”€å”®æ•°æ®",
            "context": PluginContext(
                agent_id="data_analyzer",
                task_id="analysis_task",
                session_id="demo_session",
                data={
                    "analysis_type": "basic_stats",
                    "data": [item["sales"] for item in sales_data]
                }
            )
        },
        {
            "name": "è¶‹åŠ¿åˆ†æ - é”€å”®è¶‹åŠ¿",
            "context": PluginContext(
                agent_id="data_analyzer",
                task_id="analysis_task",
                session_id="demo_session",
                data={
                    "analysis_type": "trend_analysis",
                    "data": [{"x": i, "y": item["sales"]} for i, item in enumerate(sales_data)]
                }
            )
        },
        {
            "name": "åˆ†ç»„åˆ†æ - æŒ‰åœ°åŒºåˆ†ç»„",
            "context": PluginContext(
                agent_id="data_analyzer",
                task_id="analysis_task",
                session_id="demo_session",
                data={
                    "analysis_type": "group_analysis",
                    "data": sales_data,
                    "group_by": "region"
                }
            )
        },
        {
            "name": "å¼‚å¸¸æ£€æµ‹ - ç”¨æˆ·æ”¶å…¥",
            "context": PluginContext(
                agent_id="data_analyzer",
                task_id="analysis_task",
                session_id="demo_session",
                data={
                    "analysis_type": "anomaly_detection",
                    "data": [item["income"] for item in user_data]
                }
            )
        },
        {
            "name": "ç›¸å…³æ€§åˆ†æ - å¹´é¾„vsæ”¶å…¥vsæ»¡æ„åº¦",
            "context": PluginContext(
                agent_id="data_analyzer",
                task_id="analysis_task",
                session_id="demo_session",
                data={
                    "analysis_type": "correlation",
                    "data": user_data
                }
            )
        },
        {
            "name": "æ—¶é—´åºåˆ—åˆ†æ",
            "context": PluginContext(
                agent_id="data_analyzer",
                task_id="analysis_task",
                session_id="demo_session",
                data={
                    "analysis_type": "time_series",
                    "data": time_series_data
                }
            )
        }
    ]
    
    print(f"\nğŸ¯ å¼€å§‹æ‰§è¡Œ {len(analysis_tasks)} ä¸ªåˆ†æä»»åŠ¡ï¼š")
    print("-" * 40)
    
    # æ‰§è¡Œæ‰€æœ‰åˆ†æä»»åŠ¡
    for i, task in enumerate(analysis_tasks, 1):
        print(f"\n{i}. {task['name']}ï¼š")
        result = await analyzer.execute_task(task["context"])
        
        if result.get("success"):
            print(f"   âœ… åˆ†æå®Œæˆ (è€—æ—¶ï¼š{result.get('processing_time', 'N/A')})")
            
            # æ˜¾ç¤ºå…³é”®ç»“æœ
            if "statistics" in result:
                stats = result["statistics"]
                print(f"      ğŸ“Š ç»Ÿè®¡ç»“æœï¼šå¹³å‡å€¼={stats['mean']:.2f}, ä¸­ä½æ•°={stats['median']:.2f}")
            
            elif "trend" in result:
                trend = result["trend"]
                print(f"      ğŸ“ˆ è¶‹åŠ¿ï¼š{trend['direction']}, å˜åŒ–ç‡={trend['change_percentage']:.1f}%")
            
            elif "group_analysis" in result:
                groups = result["group_analysis"]
                print(f"      ğŸ“Š åˆ†ç»„ï¼š{result['total_groups']} ä¸ªç»„")
                for group_name, group_data in list(groups.items())[:2]:  # åªæ˜¾ç¤ºå‰2ç»„
                    print(f"         - {group_name}: {group_data['count']} é¡¹")
            
            elif "anomaly_detection" in result:
                anomaly = result["anomaly_detection"]
                print(f"      ğŸš¨ å¼‚å¸¸ï¼š{anomaly['total_anomalies']} ä¸ª ({anomaly['anomaly_rate']:.1f}%)")
            
            elif "correlation_analysis" in result:
                correlations = result["correlation_analysis"]
                print(f"      ğŸ”— ç›¸å…³æ€§ï¼š{len(correlations)} å¯¹å­—æ®µ")
                for pair, data in list(correlations.items())[:2]:  # æ˜¾ç¤ºå‰2å¯¹
                    print(f"         - {pair}: {data['strength']} ({data['correlation']:.3f})")
            
            elif "time_series_analysis" in result:
                ts = result["time_series_analysis"]
                print(f"      ğŸ“… æ—¶åºï¼š{ts['data_points']} ä¸ªç‚¹, è¶‹åŠ¿={ts['value_trend']}")
        
        else:
            print(f"   âŒ åˆ†æå¤±è´¥ï¼š{result.get('error', 'æœªçŸ¥é”™è¯¯')}")
    
    # æ˜¾ç¤ºæ€»ç»“
    print("\n" + "=" * 60)
    summary = analyzer.get_analysis_summary()
    print("ğŸ“Š åˆ†ææ€»ç»“ï¼š")
    for key, value in summary.items():
        if key == "analysis_types_used":
            print(f"   {key}: {', '.join(value)}")
        elif isinstance(value, float):
            print(f"   {key}: {value:.2f}")
        else:
            print(f"   {key}: {value}")
    
    await analyzer.cleanup()


if __name__ == "__main__":
    asyncio.run(run_analysis_demo())
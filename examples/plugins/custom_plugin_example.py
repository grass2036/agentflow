#!/usr/bin/env python3
"""
æ’ä»¶å¼€å‘ç¤ºä¾‹ï¼šè‡ªå®šä¹‰æ’ä»¶
========================

è¿™ä¸ªç¤ºä¾‹å±•ç¤ºå¦‚ä½•åˆ›å»ºè‡ªå®šä¹‰æ’ä»¶ï¼ŒåŒ…æ‹¬å®Œæ•´çš„ç”Ÿå‘½å‘¨æœŸç®¡ç†ã€‚

è¿è¡Œæ–¹å¼ï¼š
python3 examples/plugins/custom_plugin_example.py
"""

import asyncio
import json
import sys
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from agentflow.plugins.base import BasePlugin, PluginMetadata, PluginContext
from agentflow.plugins.manager import plugin_manager


class DataProcessorPlugin(BasePlugin):
    """æ•°æ®å¤„ç†æ’ä»¶ç¤ºä¾‹"""
    
    @property
    def metadata(self) -> PluginMetadata:
        return PluginMetadata(
            name="data_processor",
            version="1.2.0",
            description="å¼ºå¤§çš„æ•°æ®å¤„ç†å’Œåˆ†ææ’ä»¶",
            author="AgentFlow Examples",
            homepage="https://github.com/agentflow/examples",
            tags=["data", "processing", "analysis", "custom"],
            supports_async=True
        )
    
    def __init__(self, config=None):
        super().__init__(config)
        self.processed_count = 0
        self.error_count = 0
        self.data_cache = {}
        self.operations_log = []
    
    async def initialize(self) -> None:
        """æ’ä»¶åˆå§‹åŒ–"""
        print(f"ğŸ”Œ {self.metadata.name} v{self.metadata.version} åˆå§‹åŒ–ä¸­...")
        
        # æ³¨å†Œæ”¯æŒçš„é’©å­
        self.register_hook("process_data")
        self.register_hook("validate_data")
        self.register_hook("transform_data")
        
        # è®¾ç½®é»˜è®¤é…ç½®
        self.max_cache_size = self.config.config.get("max_cache_size", 100)
        self.enable_logging = self.config.config.get("enable_logging", True)
        
        print(f"   âœ“ æœ€å¤§ç¼“å­˜ï¼š{self.max_cache_size}")
        print(f"   âœ“ æ—¥å¿—è®°å½•ï¼š{self.enable_logging}")
        print(f"   âœ“ æ”¯æŒçš„é’©å­ï¼š{', '.join(self.get_hooks())}")
        print(f"ğŸŸ¢ {self.metadata.name} åˆå§‹åŒ–å®Œæˆï¼")
    
    async def execute_task(self, context: PluginContext) -> Dict[str, Any]:
        """æ‰§è¡Œæ•°æ®å¤„ç†ä»»åŠ¡"""
        operation = context.data.get("operation", "unknown")
        data = context.data.get("data")
        
        if not data:
            return {"error": "æ²¡æœ‰æä¾›æ•°æ®", "success": False}
        
        try:
            # è®°å½•æ“ä½œå¼€å§‹
            start_time = datetime.now()
            
            # æ ¹æ®æ“ä½œç±»å‹å¤„ç†æ•°æ®
            result = await self._process_by_operation(operation, data, context)
            
            # è®°å½•æ“ä½œç»“æœ
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            
            self.processed_count += 1
            
            if self.enable_logging:
                self.operations_log.append({
                    "timestamp": start_time.isoformat(),
                    "operation": operation,
                    "duration": duration,
                    "success": True,
                    "data_size": len(str(data))
                })
            
            result.update({
                "success": True,
                "processed_count": self.processed_count,
                "duration": f"{duration:.3f}s"
            })
            
            return result
            
        except Exception as e:
            self.error_count += 1
            error_result = {
                "error": str(e),
                "success": False,
                "error_count": self.error_count,
                "operation": operation
            }
            
            if self.enable_logging:
                self.operations_log.append({
                    "timestamp": datetime.now().isoformat(),
                    "operation": operation,
                    "error": str(e),
                    "success": False
                })
            
            return error_result
    
    async def _process_by_operation(self, operation: str, data: Any, context: PluginContext) -> Dict[str, Any]:
        """æ ¹æ®æ“ä½œç±»å‹å¤„ç†æ•°æ®"""
        
        if operation == "count":
            # è®¡æ•°æ“ä½œ
            if isinstance(data, (list, dict, str)):
                count = len(data)
                return {"operation": "count", "result": count, "data_type": type(data).__name__}
            else:
                return {"operation": "count", "result": 1, "data_type": type(data).__name__}
        
        elif operation == "sum":
            # æ±‚å’Œæ“ä½œ
            if isinstance(data, list) and all(isinstance(x, (int, float)) for x in data):
                total = sum(data)
                return {"operation": "sum", "result": total, "items_count": len(data)}
            else:
                raise ValueError("æ±‚å’Œæ“ä½œéœ€è¦æ•°å­—åˆ—è¡¨")
        
        elif operation == "filter":
            # è¿‡æ»¤æ“ä½œ
            filter_condition = context.data.get("condition", {})
            if isinstance(data, list) and isinstance(filter_condition, dict):
                filtered = self._filter_data(data, filter_condition)
                return {
                    "operation": "filter",
                    "result": filtered,
                    "original_count": len(data),
                    "filtered_count": len(filtered)
                }
            else:
                raise ValueError("è¿‡æ»¤æ“ä½œéœ€è¦åˆ—è¡¨æ•°æ®å’Œæ¡ä»¶å­—å…¸")
        
        elif operation == "group":
            # åˆ†ç»„æ“ä½œ
            group_key = context.data.get("group_by")
            if isinstance(data, list) and group_key:
                grouped = self._group_data(data, group_key)
                return {
                    "operation": "group",
                    "result": grouped,
                    "groups_count": len(grouped),
                    "group_by": group_key
                }
            else:
                raise ValueError("åˆ†ç»„æ“ä½œéœ€è¦åˆ—è¡¨æ•°æ®å’Œåˆ†ç»„é”®")
        
        elif operation == "validate":
            # éªŒè¯æ“ä½œ
            schema = context.data.get("schema", {})
            validation_result = self._validate_data(data, schema)
            return {
                "operation": "validate",
                "result": validation_result,
                "is_valid": validation_result["is_valid"]
            }
        
        elif operation == "cache":
            # ç¼“å­˜æ“ä½œ
            cache_key = context.data.get("key", f"cache_{len(self.data_cache)}")
            self._manage_cache(cache_key, data)
            return {
                "operation": "cache",
                "result": f"æ•°æ®å·²ç¼“å­˜åˆ°é”®ï¼š{cache_key}",
                "cache_size": len(self.data_cache)
            }
        
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ“ä½œï¼š{operation}")
    
    def _filter_data(self, data: list, condition: dict) -> list:
        """è¿‡æ»¤æ•°æ®"""
        filtered = []
        for item in data:
            if isinstance(item, dict):
                match = True
                for key, value in condition.items():
                    if key not in item or item[key] != value:
                        match = False
                        break
                if match:
                    filtered.append(item)
            elif hasattr(item, '__dict__'):
                # å¯¹è±¡è¿‡æ»¤
                match = True
                for key, value in condition.items():
                    if not hasattr(item, key) or getattr(item, key) != value:
                        match = False
                        break
                if match:
                    filtered.append(item)
        return filtered
    
    def _group_data(self, data: list, group_key: str) -> dict:
        """åˆ†ç»„æ•°æ®"""
        groups = {}
        for item in data:
            if isinstance(item, dict) and group_key in item:
                key = item[group_key]
            elif hasattr(item, group_key):
                key = getattr(item, group_key)
            else:
                key = "unknown"
            
            if key not in groups:
                groups[key] = []
            groups[key].append(item)
        
        return groups
    
    def _validate_data(self, data: Any, schema: dict) -> dict:
        """éªŒè¯æ•°æ®"""
        errors = []
        
        # ç®€å•çš„ç±»å‹éªŒè¯
        expected_type = schema.get("type")
        if expected_type:
            if expected_type == "list" and not isinstance(data, list):
                errors.append(f"æœŸæœ›ç±»å‹ listï¼Œå®é™…ç±»å‹ {type(data).__name__}")
            elif expected_type == "dict" and not isinstance(data, dict):
                errors.append(f"æœŸæœ›ç±»å‹ dictï¼Œå®é™…ç±»å‹ {type(data).__name__}")
            elif expected_type == "str" and not isinstance(data, str):
                errors.append(f"æœŸæœ›ç±»å‹ strï¼Œå®é™…ç±»å‹ {type(data).__name__}")
        
        # å¿…éœ€å­—æ®µéªŒè¯
        required_fields = schema.get("required", [])
        if isinstance(data, dict):
            for field in required_fields:
                if field not in data:
                    errors.append(f"ç¼ºå°‘å¿…éœ€å­—æ®µï¼š{field}")
        
        # é•¿åº¦éªŒè¯
        min_length = schema.get("min_length")
        max_length = schema.get("max_length")
        if hasattr(data, '__len__'):
            data_len = len(data)
            if min_length and data_len < min_length:
                errors.append(f"é•¿åº¦ {data_len} å°äºæœ€å°é•¿åº¦ {min_length}")
            if max_length and data_len > max_length:
                errors.append(f"é•¿åº¦ {data_len} å¤§äºæœ€å¤§é•¿åº¦ {max_length}")
        
        return {
            "is_valid": len(errors) == 0,
            "errors": errors,
            "schema_applied": schema
        }
    
    def _manage_cache(self, key: str, data: Any):
        """ç®¡ç†ç¼“å­˜"""
        # å¦‚æœç¼“å­˜æ»¡äº†ï¼Œåˆ é™¤æœ€æ—§çš„é¡¹
        if len(self.data_cache) >= self.max_cache_size:
            oldest_key = next(iter(self.data_cache))
            del self.data_cache[oldest_key]
        
        self.data_cache[key] = {
            "data": data,
            "timestamp": datetime.now().isoformat(),
            "access_count": 0
        }
    
    def get_cache(self, key: str) -> Optional[Any]:
        """è·å–ç¼“å­˜æ•°æ®"""
        if key in self.data_cache:
            self.data_cache[key]["access_count"] += 1
            return self.data_cache[key]["data"]
        return None
    
    def get_statistics(self) -> Dict[str, Any]:
        """è·å–æ’ä»¶ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "processed_count": self.processed_count,
            "error_count": self.error_count,
            "success_rate": (self.processed_count / (self.processed_count + self.error_count) * 100) 
                          if (self.processed_count + self.error_count) > 0 else 0,
            "cache_size": len(self.data_cache),
            "operations_logged": len(self.operations_log),
            "supported_operations": [
                "count", "sum", "filter", "group", "validate", "cache"
            ]
        }
    
    async def cleanup(self) -> None:
        """æ’ä»¶æ¸…ç†"""
        print(f"\nğŸ§¹ {self.metadata.name} å¼€å§‹æ¸…ç†...")
        
        stats = self.get_statistics()
        print(f"   ğŸ“Š å¤„ç†ç»Ÿè®¡ï¼š")
        print(f"      - æˆåŠŸå¤„ç†ï¼š{stats['processed_count']} æ¬¡")
        print(f"      - é”™è¯¯æ¬¡æ•°ï¼š{stats['error_count']} æ¬¡")
        print(f"      - æˆåŠŸç‡ï¼š{stats['success_rate']:.1f}%")
        print(f"      - ç¼“å­˜å¤§å°ï¼š{stats['cache_size']}")
        
        # æ¸…ç†èµ„æº
        self.data_cache.clear()
        self.operations_log.clear()
        
        print(f"ğŸ”´ {self.metadata.name} æ¸…ç†å®Œæˆ")


async def demo_plugin_usage():
    """æ¼”ç¤ºæ’ä»¶ä½¿ç”¨"""
    print("ğŸŒŠ AgentFlow è‡ªå®šä¹‰æ’ä»¶å¼€å‘ç¤ºä¾‹")
    print("=" * 60)
    
    # åˆ›å»ºæ’ä»¶å®ä¾‹
    from agentflow.plugins.base import PluginConfig
    config = PluginConfig(
        enabled=True,
        config={
            "max_cache_size": 50,
            "enable_logging": True
        }
    )
    plugin = DataProcessorPlugin(config)
    
    try:
        # åˆå§‹åŒ–æ’ä»¶
        await plugin.initialize()
        
        # æµ‹è¯•ç”¨ä¾‹
        test_cases = [
            {
                "name": "è®¡æ•°æ“ä½œ",
                "context": PluginContext(
                    agent_id="data_processor",
                    task_id="task_count",
                    session_id="demo_session",
                    data={
                        "operation": "count",
                        "data": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
                    }
                )
            },
            {
                "name": "æ±‚å’Œæ“ä½œ",
                "context": PluginContext(
                    agent_id="data_processor",
                    task_id="task_sum",
                    session_id="demo_session",
                    data={
                        "operation": "sum",
                        "data": [10, 20, 30, 40, 50]
                    }
                )
            },
            {
                "name": "è¿‡æ»¤æ“ä½œ",
                "context": PluginContext(
                    agent_id="data_processor",
                    task_id="task_sum",
                    session_id="demo_session",
                    data={
                        "operation": "filter",
                        "data": [
                            {"name": "Alice", "age": 25, "city": "åŒ—äº¬"},
                            {"name": "Bob", "age": 30, "city": "ä¸Šæµ·"},
                            {"name": "Charlie", "age": 25, "city": "å¹¿å·"},
                            {"name": "Diana", "age": 28, "city": "åŒ—äº¬"}
                        ],
                        "condition": {"city": "åŒ—äº¬"}
                    }
                )
            },
            {
                "name": "åˆ†ç»„æ“ä½œ",
                "context": PluginContext(
                    agent_id="data_processor",
                    task_id="task_sum",
                    session_id="demo_session",
                    data={
                        "operation": "group",
                        "data": [
                            {"name": "äº§å“A", "category": "ç”µå­", "price": 100},
                            {"name": "äº§å“B", "category": "æœè£…", "price": 50},
                            {"name": "äº§å“C", "category": "ç”µå­", "price": 200},
                            {"name": "äº§å“D", "category": "æœè£…", "price": 80}
                        ],
                        "group_by": "category"
                    }
                )
            },
            {
                "name": "æ•°æ®éªŒè¯",
                "context": PluginContext(
                    agent_id="data_processor",
                    task_id="task_sum",
                    session_id="demo_session",
                    data={
                        "operation": "validate",
                        "data": {"name": "æµ‹è¯•ç”¨æˆ·", "email": "test@example.com"},
                        "schema": {
                            "type": "dict",
                            "required": ["name", "email"],
                            "min_length": 1
                        }
                    }
                )
            },
            {
                "name": "ç¼“å­˜æ“ä½œ",
                "context": PluginContext(
                    agent_id="data_processor",
                    task_id="task_sum",
                    session_id="demo_session",
                    data={
                        "operation": "cache",
                        "key": "user_data",
                        "data": {"user_id": 123, "preferences": {"theme": "dark"}}
                    }
                )
            }
        ]
        
        print(f"\nğŸ¯ å¼€å§‹æ‰§è¡Œ {len(test_cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹ï¼š")
        print("-" * 40)
        
        # æ‰§è¡Œæ‰€æœ‰æµ‹è¯•ç”¨ä¾‹
        for i, test_case in enumerate(test_cases, 1):
            print(f"\n{i}. {test_case['name']}ï¼š")
            result = await plugin.execute_task(test_case["context"])
            
            if result.get("success"):
                print(f"   âœ… æˆåŠŸï¼š{result.get('result', 'æ— ç»“æœ')}")
                if "duration" in result:
                    print(f"   â±ï¸  è€—æ—¶ï¼š{result['duration']}")
            else:
                print(f"   âŒ å¤±è´¥ï¼š{result.get('error', 'æœªçŸ¥é”™è¯¯')}")
        
        # æµ‹è¯•ç¼“å­˜è·å–
        print(f"\nğŸ—„ï¸ æµ‹è¯•ç¼“å­˜è·å–ï¼š")
        cached_data = plugin.get_cache("user_data")
        if cached_data:
            print(f"   âœ… ç¼“å­˜å‘½ä¸­ï¼š{cached_data}")
        else:
            print(f"   âŒ ç¼“å­˜æœªå‘½ä¸­")
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        stats = plugin.get_statistics()
        print(f"\nğŸ“Š æ’ä»¶æ‰§è¡Œç»Ÿè®¡ï¼š")
        for key, value in stats.items():
            if key == "supported_operations":
                print(f"   {key}: {', '.join(value)}")
            else:
                print(f"   {key}: {value}")
        
    finally:
        # æ¸…ç†æ’ä»¶
        await plugin.cleanup()


if __name__ == "__main__":
    asyncio.run(demo_plugin_usage())